{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad278363",
   "metadata": {},
   "source": [
    "# Understanding Decoder‑Only Transformers: A Token‑Level Journey\n",
    "\n",
    "While there are many excellent overviews of decoder‑only architectures, I found most leave out the deeper, token‑level intuition (or at least I didn’t get it while reading them). I’d been following each multiplication and decision mechanically without fully grasping what was happening under the hood since many tutorials don’t show shapes explicily or think matrix multiplications are intuitive.\n",
    "\n",
    "This notebook aims to change that. It dives into every token-level transformation, showing shapes, intermediate vectors, and how information flows through a decoder‑only stack. So if you think you already know the high level stuff but catch yourself beeing usure when taking stuff a level deeper this might be just right for you. \n",
    "\n",
    "Hopefully, this walk‑through will spark the same “aha!” moments it did for me. If not and you still don’t fully grasp it, i strongly suggest you make such a notebook yourself as this was a great way of learning the stuff!\n",
    "\n",
    "Like any other insight out there, take it with a grain of salt! \n",
    "\n",
    "Feel free to openup an issue if i got something wrong im myself not an expert but trying to get somewhere decent! \n",
    "\n",
    "___\n",
    "___\n",
    "\n",
    "# Design Parameters of the GPT\n",
    "\n",
    "___\n",
    "___\n",
    "\n",
    "- **dim embd/dim model** (denote as $d$): The size of our embeddings also called the model’s (hidden) size\n",
    "- **vocab size** (denote as $v$): The size of our vocabulary (number of different input tokens)\n",
    "- **context length** (denote as $c$): The maximal context length (input length in tokens) we can feed our model\n",
    "- **n heads** (denote as $H$): The number of attention heads in each transformer block\n",
    "- **n layers** (denote as $N$): The number of transformer blocks\n",
    "___\n",
    "___\n",
    "\n",
    "# From Token IDs to Input Embeddings\n",
    "___\n",
    "___\n",
    "\n",
    "*Note:* We will omit the batch dimension for simplicity thereby all tensors represent a single sequence!\n",
    "\n",
    "### What do we have and what does it look like?\n",
    "\n",
    "- **Input token sequence**: $\\mathbf{I}_{\\mathrm{ids}} \\in \\mathbb{R}^{c}$ — this is our input vector of (maximal) length $c$ containing the token IDs.\n",
    "- **Token embedding matrix**: $\\mathbf{E}_{\\text{token}} \\in \\mathbb{R}^{v \\times d}$ - this is our learned embedding matrix which we use as lookup table to map each of the $v$ vocabulary items to a $d$-dimensional embedding vector.\n",
    "- **Positional embedding matrix**: $\\mathbf{E}_{\\text{pos}} \\in \\mathbb{R}^{c \\times d}$ - this is our learned positional embedding matrix. It provides a $d$-dimensional vector that encodes positional information for each of the $c$ positions in our input.\n",
    "\n",
    "### What happens here?\n",
    "\n",
    "1. Use $\\mathbf{I}_{\\mathrm{ids}}$ to index into $\\mathbf{E}_{\\text{token}}$, resulting in:  \n",
    "   $I_{\\mathrm{emb}} \\in \\mathbb{R}^{c \\times d}$  \n",
    "2. Add the positional embeddings elementwise:  \n",
    "   $I_{\\mathrm{emb}} = I_{\\mathrm{emb}} + \\mathbf{E}_{\\text{pos}}$\n",
    "\n",
    "### What do we end up with?\n",
    "\n",
    "After this step, each token is represented by a vector that encodes both **its identity and position**:  \n",
    "$I_{\\mathrm{emb}} \\in \\mathbb{R}^{c \\times d}$\n",
    "\n",
    "### Why add position information this way?\n",
    "\n",
    "It may seem unintuitive to **add** positional vectors instead of appending them. After all, the sum of a token embedding and its positional vector could, in theory, be identical to the sum of a different token and a different position — leading to ambiguous representations. *(That was my initial concern.)*\n",
    "\n",
    "But actually, spreading positional information across **all $d$ dimensions** allows every attention head to access positional context. Appending would isolate position into a subspace — which weakens its utility. Additionally as the model learns both the token embeddings and the positional embeddings it can itself make sure that it can seperate each of these combinations in a meaningful way.\n",
    "___\n",
    "___\n",
    "\n",
    "# The $N$ Transformer Blocks\n",
    "\n",
    "Now that we have our embedded input, we move into the **transformer blocks**, the core component of the architecture. In the original GPT-1 paper [<cite>Radford, Narasimhan, Salimans & Sutskever (2018)</cite>](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) they used 12 such transformer blocks. We will discuss the block structure they proposed which places the LayerNorm different than originally proposed by [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "___\n",
    "___\n",
    "\n",
    "each of these blocks has the same structure. We will dive into each of them in order such that we can follow the manipulations in detail. \n",
    "*Note:* In the subsequent descriptions i will use a notation that only makes sense if we talk about the first block right after embedding the tokens. That way i think its more intuitive and easier to follow but when looking at this more general you have to keep in mind that the input is of couse not the initially embedded input but the output of the previous block!\n",
    "\n",
    "1. **Masked Multi-Head Self-Attention**\n",
    "2. **Residual Connection**  \n",
    "3. **LayerNorm**  \n",
    "4. **Feedforward Network (MLP)**  \n",
    "5. **Residual Connection**\n",
    "6. **LayerNorm**  \n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# 1: Masked Multihead Self Attention (MHA)\n",
    "\n",
    "[Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Recall: after the embedding step, our input is:  \n",
    "$I_{\\mathrm{emb}} \\in \\mathbb{R}^{c \\times d}$\n",
    "\n",
    "\n",
    "### First step – Query $Q$, Key $K$, and Value $V$ calculation\n",
    "\n",
    "The goal of this step is to transform each token’s embedding into three distinct representations *query*, *key*, and *value* which serve complementary roles. The *query* and *key* allow us to measure how relevant other tokens are, and the *value* contains the content to be pooled if that relevance is high. At this stage, the transformations are still applied token-wise and no interactions between tokens have occurred yet.\n",
    "\n",
    "1. **Linear projections and head splitting**\n",
    "\n",
    "    In Multi-Head Attention, we split the total model dimension $d$ into $H$ smaller parts, one per head, each of size $d_{head}$, typically with $d_{head} = d / H$.\n",
    "\n",
    "    There are two equivalent ways to compute the  *query*, *key*, and *value* projections for each head:\n",
    "\n",
    "    - **Single large projection + slicing** (commonly used in practice):  \n",
    "        A single learned projection $W^Q \\in \\mathbb{R}^{d \\times (H \\cdot d_{head})}$ transforms the input into one large matrix, which is then reshaped into $H$ individual heads:\n",
    "\n",
    "        $$\n",
    "        Q = I_{\\mathrm{emb}} W^Q \\in \\mathbb{R}^{c \\times (H \\cdot d_{head})} \\quad \\Rightarrow \\quad Q \\in \\mathbb{R}^{c \\times H \\times d_{head}}\n",
    "        $$\n",
    "\n",
    "        Even though this projection is implemented as a single large matrix, each head operates only on its own slice of the output. During training, the attention computed within each head only influences the corresponding slice of $W^Q$, so each part of the projection matrix is still **effectively head-specific** in how it learns and updates. The same structure applies to $K$ and $V$.\n",
    "\n",
    "\n",
    "    - **Separate projection per head** (conceptually clearer):  \n",
    "        Each head has its own smaller projection matrix, $W^Q_h \\in \\mathbb{R}^{d \\times d_{head}}$, applied independently to the input to compute $Q_h \\in \\mathbb{R}^{c \\times d_{head}}$ for head $h$.\n",
    "\n",
    "        Both approaches produce the same mathematical result: a tensor of shape $\\mathbb{R}^{c \\times H \\times d_{head}}$ for each of $Q, K, V$, where each token now has multiple versions of itself—one per head.\n",
    "\n",
    "2. **Choosing the per-head dimension $d_{head}$**\n",
    "\n",
    "    - **Balanced capacity**: Often $d_{head} = d / H$, keeping the total number of parameters and computation constant across different numbers of heads.\n",
    "    - **Trade-offs**: Larger $d_{head}$ per head can model more complex relationships, but increases cost; smaller $d_{head}$ is cheaper but may underrepresent fine-grained patterns.\n",
    "\n",
    "3. **Token-level intuition**\n",
    "\n",
    "    - Each row of $Q$, $K$, and $V$ corresponds to a token representation—now split across multiple heads.\n",
    "    - The **query** vector encodes what this token is \"looking for\" in the sequence.\n",
    "    - The **key** vector encodes what this token \"offers\" to other queries.\n",
    "    - The **value** vector contains the actual content to be passed along if the match (query–key similarity) is strong.\n",
    "\n",
    "\n",
    "### Second step – Raw Attention score calculation\n",
    "\n",
    "Once we have the projected representations (per head) $Q, K \\in \\mathbb{R}^{c \\times d_{head}}$, we compute the attention scores by taking the dot product between each query and all keys, followed by scaling. This results in a score matrix $A \\in \\mathbb{R}^{c \\times c}$, where each entry $A_{i,j}$ represents the unnormalized relevance of token $j$ to token $i$.\n",
    "\n",
    "Formally, the attention score matrix is computed as:\n",
    "\n",
    "$$\n",
    "A = \\frac{Q K^{\\top}}{\\sqrt{d_{head}}} \\in \\mathbb{R}^{c \\times c}\n",
    "$$\n",
    "\n",
    "Since the dot product can produce pretty big numbers if the dimensionality $d_{head}$ is large, we divide by $\\sqrt{d_{head}}$ to counteract this growth. This both stabilizes the gradients and thereby learning but also makes the later applied softmax outcomes less peaky and helps distribute the attention better over the different tokens. \n",
    "\n",
    "By now each row in $A$ contains the (scaled) attention logits for a single token’s query vector against all other tokens' key vectors. For example, *A[2][5]* is the scaled dot product between the query vector of token $2$ and the key vector of token $5$. These scores are still unnormalized and are sometimes referred to as the \"raw\" attention scores.\n",
    "\n",
    "### Third step – Causal Masking\n",
    "\n",
    "In decoder-only architectures like GPT, masking is a required step. It ensures that **each token can only attend to itself and preceding tokens** never to future ones. This preserves the autoregressive property of the model, where predictions are generated left-to-right without *peeking* ahead.\n",
    "\n",
    "To enforce this, we apply a **causal mask** to the attention score matrix $A$ before the softmax. The mask suppresses all attention to future tokens by adding large negative values (effectively $-\\infty$) to those positions:\n",
    "\n",
    "Define the mask:\n",
    "\n",
    "$$\n",
    "M_{i,j} = \\begin{cases}\n",
    "0, & j \\le i; \\\\\\\\\n",
    "-\\infty, & j > i,\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and set:\n",
    "\n",
    "$$\n",
    "A_{\\text{masked}} = A + M \\in \\mathbb{R}^{c \\times c}\n",
    "$$\n",
    "\n",
    "This ensures that when softmax is applied, all entries corresponding to future tokens have probability zero, and each token attends only to itself and earlier tokens in the sequence.\n",
    "\n",
    "### Fourth step - Softmax\n",
    "\n",
    "After masking, we apply the softmax function to each row of the attention score matrix  $A_{\\text{masked}}$. This converts the raw (and masked) scores into a probability distribution over the input tokens:\n",
    "\n",
    "$$\n",
    "\\alpha_{i,j} = \\frac{\\exp(A_{\\text{masked}, i,j})}{\\sum_{k=1}^{c} \\exp(A_{\\text{masked}, i,k})}\n",
    "$$\n",
    "\n",
    "This gives us the **attention weight matrix** $\\alpha \\in \\mathbb{R}^{c \\times c}$, where each row $\\alpha_i$ represents how much token $i$ attends to every other token.\n",
    "\n",
    "### Fifth step - Context calculation\n",
    "\n",
    "These attention weights are then used to compute a **weighted sum of the value vectors**:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\alpha V \\in \\mathbb{R}^{c \\times d_{head}}\n",
    "$$\n",
    "\n",
    "The result is a new representation for each token, enriched by selectively aggregating information from the other tokens based on relevance.\n",
    "\n",
    "Each token now has access to context-dependent information, shaped by the learned query-key matching and the value vectors it attends to.\n",
    "\n",
    "### Final step – Combining the heads\n",
    "\n",
    "So far, each of the $h$ attention heads has produced its own context output of shape $\\mathbb{R}^{c \\times d_{head}}$. To merge these into a single representation per token, we **concatenate** the outputs along the feature dimension:\n",
    "\n",
    "$$\n",
    "\\text{ConcatHead} \\in \\mathbb{R}^{c \\times (H \\cdot d_{head})} = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_H)\n",
    "$$\n",
    "\n",
    "Then, we apply a final learned linear transformation $W^O \\in \\mathbb{R}^{(H \\cdot d_{head}) \\times d}$ to project this concatenated representation back into the model’s embedding space:\n",
    "\n",
    "$$\n",
    "\\text{MHA}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_H) \\cdot W^O \\in \\mathbb{R}^{c \\times d}\n",
    "$$\n",
    "\n",
    "The final projection matrix $W^O \\in \\mathbb{R}^{(H \\cdot d_{head}) \\times d}$ is applied independently to each token’s concatenated head output. It does **not** mix information across different tokens—each token is processed separately.\n",
    "\n",
    "What it does do is mix information **across heads**: since each token’s vector includes all $H$ heads stacked into a single $\\mathbb{R}^{H \\cdot d_{head}}$ vector, the projection learns how to combine and weight the contributions from each head. This is the only step where information from the different heads is fused into a single representation per token.\n",
    "\n",
    "### Some final thoughts on multihead self attention\n",
    "\n",
    "I initially questioned the need for explicitly splitting attention into multiple heads. From a mathematical standpoint, it seemed plausible that a single-head could learn to capture everything needed by selectively focusing on different aspects of the input. However, a key limitation emerges when we consider how attention behaves in practice.\n",
    "\n",
    "Imagine one token is highly relevant to another; in a single-head setup, this dominant relevance would likely cause the attention mechanism to concentrate most, if not all, of the attention weight on that single token, effectively zeroing out contributions from others. This creates a bottleneck: the model is forced to choose one narrow interpretation or interaction per layer.\n",
    "\n",
    "Multi-head attention addresses this by allowing the model to attend to information from different representation subspaces simultaneously. Each head learns to focus on different types of relationships (syntactic, semantic, positional, or otherwise) without competing for a single attention budget.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# 2: Residual Connection\n",
    "[He et al., 2015](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Recall: after the Multihead self attention, our input is still in the same shape but now the representations of the individual tokens are not solely based on our learned embeddings and positions but the representation of each token is enriched with information of all the other tokens that come before it. We will use the following notation for the output of the Multihead self attention:\n",
    "$I_{\\mathrm{MHA}} \\in \\mathbb{R}^{c \\times d}$\n",
    "\n",
    "### What do we do here?\n",
    "\n",
    "The residual connection is conceptually and mathematically quite simple (but as we later see has multiple nice effects and rationales). We just add the input of the previous layer to the output of the previous layer.\n",
    "\n",
    "So more generally stated:\n",
    "\n",
    "If $x$ is the input to a sub-layer and $\\mathrm{Sublayer}(x)$ its transformation, the output becomes:\n",
    "\n",
    "\n",
    "$$\n",
    "y = x + \\mathrm{Sublayer}(x)\n",
    "$$\n",
    "\n",
    "### So what are the effects and rationales?\n",
    "\n",
    "- **Improved gradient flow**: Residual connections **create a shortcut path** that **allows gradients to bypass** the inner operations of a sub-layer—such as multi-head attention or a feedforward block—during backpropagation. More precisely, they make it possible to compute gradient updates for earlier layers without the gradients having to pass entirely through the potentially unstable or compressive operations inside the sub-layer. This **reduces the risk of gradients becoming extremely small** due to repeated matrix multiplications, making it easier to train deep networks effectively.\n",
    "\n",
    "- **Modeling residual functions**: Instead of learning the full output mapping  $\\mathcal{F}(x) = y$, residual connections reframe the problem as learning the *difference* between the input and the desired output:\n",
    "\n",
    "$$\n",
    "\\mathcal{F}(x) = y - x \\quad \\Rightarrow \\quad y = x + \\mathcal{F}(x)\n",
    "$$\n",
    "\n",
    "This means that the sub-layer only needs to learn how to adjust or refine the input $x$, rather than construct $y$ from scratch. If no change is needed, $\\mathcal{F}(x)$ can simply output zero, and the identity mapping is preserved. Which of couse is much easier to learn than forcing a layer to reconstruct the input through transformations.\n",
    "\n",
    "___\n",
    "___\n",
    "\n",
    "# 3: LayerNorm\n",
    "[<cite>Ba, Kiros & Hinton (2016)</cite>](https://arxiv.org/abs/1607.06450) \n",
    "___\n",
    "___\n",
    "\n",
    "Recall: after the Multihead self attention and the residual connection, our input is still in the following shape:\n",
    "$I_{\\mathrm{MHA}} \\in \\mathbb{R}^{c \\times d}$\n",
    "\n",
    "This is a matrix with one $d$-dimensional row vector per token.\n",
    "\n",
    "### What do we do here?\n",
    "\n",
    "The Layer Normalization is applied rowwise, so **independently to each token vector**. It:\n",
    "\n",
    "- Normalizes that vector to have **zero mean and unit variance**  \n",
    "- Applies a learned scale ($\\gamma$) and shift ($\\beta$)\n",
    "\n",
    "### Token-Level\n",
    "\n",
    "Let $\\mathbf{x}_i = I_{\\mathrm{MHA_i}} \\in \\mathbb{R}^d$ be the $i$ -th token vector (the $i$ -th row of $I_{\\mathrm{MHA}}\\in\\mathbb{R}^{c\\times d}$).\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\mathrm{LayerNorm}(\\mathbf{x}_i)\n",
    "=\n",
    "\\boldsymbol\\gamma\n",
    "\\;\\odot\\;\n",
    "\\frac{\\mathbf{x}_i - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}\n",
    "\\;+\\;\n",
    "\\boldsymbol\\beta\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\displaystyle \\mu_i=\\frac1d\\sum_{j=1}^d x_{i,j}$  \n",
    "- $\\displaystyle \\sigma_i^2=\\frac1d\\sum_{j=1}^d (x_{i,j}-\\mu_i)^2$  \n",
    "- $\\epsilon>0$ is a small scalar for stability  \n",
    "- $\\boldsymbol\\gamma,\\boldsymbol\\beta\\in\\mathbb{R}^d$ are _shared_ across all tokens  \n",
    "- “$\\odot$” denote element‐wise (Hadamard) product\n",
    "- The subtraction and division are applied element-wise (broadcasting the scalar $\\mu_i$ and $\\sqrt{\\sigma_i^2 + \\epsilon}$ over all $d$ dimensions),\n",
    "\n",
    "\n",
    "### Matrix-Level\n",
    "\n",
    "You can equivalently write the same per‐row normalization in one shot on the full matrix, using broadcasting.\n",
    "\n",
    "1. **Compute row‐means and variances**\n",
    "\n",
    "   Let  \n",
    "   - $I_{\\mathrm{MHA}}\\in\\mathbb{R}^{c\\times d}$  \n",
    "   - $\\mathbf{1}_d\\in\\mathbb{R}^d$ be a vector of all ones  \n",
    "   - “$\\odot$” denote element‐wise (Hadamard) product  \n",
    "\n",
    "   Then the **row‐means** $\\boldsymbol\\mu\\in\\mathbb{R}^{c\\times1}$ are\n",
    "   $$\n",
    "     \\boldsymbol\\mu\n",
    "     = \\frac{1}{d}\\;I_{\\mathrm{MHA}}\\;\\mathbf{1}_d\n",
    "     \\,,\n",
    "   $$\n",
    "   and the **row‐variances** $\\boldsymbol\\sigma^2\\in\\mathbb{R}^{c\\times1}$ are\n",
    "   $$\n",
    "     \\boldsymbol\\sigma^2\n",
    "     = \\frac{1}{d}\\,\\bigl(I_{\\mathrm{MHA}}\\odot I_{\\mathrm{MHA}}\\bigr)\\,\\mathbf{1}_d\n",
    "       \\;-\\;\\boldsymbol\\mu\\odot\\boldsymbol\\mu\n",
    "     \\,.\n",
    "   $$\n",
    "\n",
    "2. **Normalize & apply scale/shift**\n",
    "\n",
    "   Let  \n",
    "   - $\\boldsymbol\\gamma,\\boldsymbol\\beta\\in\\mathbb{R}^{d}$ be the shared learnable scale and shift vectors,  \n",
    "   - $\\epsilon>0$ a small constant for numerical stability.\n",
    "\n",
    "   Then\n",
    "   $$\n",
    "     \\mathrm{LayerNorm}\\bigl(I_{\\mathrm{MHA}}\\bigr)\n",
    "     = \\;\n",
    "     \\boldsymbol\\gamma\n",
    "     \\;\\odot\\;\n",
    "     \\frac{\\,I_{\\mathrm{MHA}}\n",
    "           \\;-\\;\\boldsymbol\\mu\\,\\mathbf{1}_d^{\\!\\top}\\,}\n",
    "          {\\sqrt{\\boldsymbol\\sigma^2 + \\epsilon}\\,\\mathbf{1}_d^{\\!\\top}}\n",
    "     \\;+\\;\n",
    "     \\boldsymbol\\beta\\,\\mathbf{1}_d^{\\!\\top}\n",
    "     \\,.\n",
    "   $$\n",
    "   - The subtraction and division of the $c\\times1$ vectors $\\boldsymbol\\mu$ and $\\sqrt{\\boldsymbol\\sigma^2+\\epsilon}$ are **broadcast** across the $d$ columns.  \n",
    "   - The vectors $\\boldsymbol\\gamma,\\boldsymbol\\beta\\in\\mathbb{R}^d$ are **broadcast** down to shape $c\\times d$ before the element‐wise multiply/add.\n",
    "\n",
    "### And why all that?\n",
    "\n",
    "Layer normalization applies a per-token normalization by subtracting each vector’s own mean and dividing by its standard deviation, which directly targets **internal covariate shift**, the phenomenon where changing parameters in earlier layers cause shifts in the distribution of later-layer inputs—thus keeping gradient magnitudes well conditioned across depth and varying sequence lengths [<cite>Ioffe & Szegedy (2015)</cite>](https://arxiv.org/abs/1502.03167). \n",
    "\n",
    "The subsequent learned affine parameters $\\boldsymbol\\gamma,\\boldsymbol\\beta$ then reintroduce per-feature scaling and bias, ensuring that the model’s representational power is fully preserved and that the network can recover any necessary distributional shape [<cite>Ba, Kiros & Hinton (2016)</cite>](https://arxiv.org/abs/1607.06450). \n",
    "\n",
    "Empirical studies have shown that this row-wise normalization smooths the loss surface and accelerates convergence, even with very small batch sizes, making *LayerNorm* especially effective in architectures such as the Transformer, where it underpins stable and efficient training of deep attention layers [<cite>Vaswani et al. (2017)</cite>](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# 4: Feed Forward Network (FFN)\n",
    "This more of a standard component derived from classical neural network architectures, rather than a novel mechanism introduced in a single paper. Nevertheless we will discuss the structure proposed by [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762). \n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Recall: at this point our input is the output of the MHA plus the input of this MHA (due to the residual connection) and then normalized with the LayerNorm.\n",
    "\n",
    "We will use the following notation for the input of this layer:\n",
    "$I_{\\mathrm{FFN}} \\in \\mathbb{R}^{c \\times d}$\n",
    "\n",
    "### What do we do and how do we do it?\n",
    "\n",
    "The FFN consists of two linear transformations with a non-linearity in between:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\mathrm{Activation}(x W_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "\n",
    "with:\n",
    "- $ W_1 \\in \\mathbb{R}^{d \\times d_{\\text{ff}}} $\n",
    "- $ W_2 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d} $\n",
    "\n",
    "Typically, $ d_{\\text{ff}} = 4d $, expanding the internal dimension significantly before projecting it back down.\n",
    "\n",
    "This transformation is applied **independently to each token**—no interaction between tokens occurs in this layer. Each row in $I_{\\mathrm{FFN}}$ is passed through the same FFN.\n",
    "\n",
    "The original Transformer used *ReLU* as the activation function, but many modern variants (e.g., GPT) use *GELU*, which is smoother and often yields better performance. The *GELU* function was introduced by [Hendrycks & Gimpel (2016)](https://arxiv.org/abs/1606.08415).\n",
    "\n",
    "### Why all that?\n",
    "\n",
    "While attention mixes information across tokens, the FFN allows the model to **process each token's enriched representation further**, giving it more capacity to model nonlinear transformations at each position.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# 5: Residual Connection\n",
    "[He et al., 2015](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Just like the first Masked Multihead Self Attention layer the FFN is also wrapped in a residual connection.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# 6: LayerNorm\n",
    "[<cite>Ba, Kiros & Hinton (2016)</cite>](https://arxiv.org/abs/1607.06450) \n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Just like the first LayerNorm we again normalize to zero mean and unit variance. \n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# Final Composition: Stacking $N$ Transformer Blocks\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Each Transformer block we've discussed operates on a full sequence of token embeddings, applying LayerNorm, attention, MLP, and residuals. In practice, the model stacks $N$ such blocks sequentially, where the output of one block becomes the input to the next.\n",
    "\n",
    "If the input to the first block is $I_0 = I_{\\text{emb}}$, then the output of block $i$ is:\n",
    "\n",
    "$$\n",
    "I_i = \\text{TransformerBlock}_i(I_{i-1})\n",
    "$$\n",
    "\n",
    "This stacking allows the model to iteratively refine each token’s representation, integrating broader context and more complex dependencies layer by layer.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# Output Projection and Language Modeling Head\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Recall: Once a token representation has passed through all $N$ transformer blocks, we obtain a final hidden state:\n",
    "\n",
    "$$\n",
    "H \\in \\mathbb{R}^{c \\times d}\n",
    "$$\n",
    "\n",
    "Each row in $H$ is the final, context-enriched representation of a token — it carries both the token’s identity and everything it has attended to across layers.\n",
    "\n",
    "### What do we do and how do we do it?\n",
    "\n",
    "To make a prediction (i.e., guess the next token), we need to convert each $d$-dimensional hidden vector into a score for each possible token in the vocabulary. This is done by applying a final linear projection:\n",
    "\n",
    "$$\n",
    "\\text{logits} = H W^T + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $W \\in \\mathbb{R}^{v \\times d}$ is the output weight matrix\n",
    "- $b \\in \\mathbb{R}^v$ is a learned bias\n",
    "- $\\text{logits} \\in \\mathbb{R}^{c \\times v}$ are the unnormalized scores for each vocabulary token at each position\n",
    "\n",
    "### From Logits to Probabilities\n",
    "\n",
    "Immediately after we get $\\text{logits}$, we apply softmax **row-wise** to obtain a probability distribution:\n",
    "\n",
    "$$\n",
    "P = \\text{softmax}(\\text{logits}) \\quad\\in\\; \\mathbb{R}^{c \\times v}\n",
    "$$\n",
    "\n",
    "- **Training**: We compare $P$ against the ground-truth “next token” at every position using cross-entropy loss.  \n",
    "- **Inference**: We typically only look at the **last row** $P_{c}\\in \\mathbb{R}^{v}$, then sample or pick the arg-max as the next token or use something more fancy like beam search.\n",
    "\n",
    "### Weight tying\n",
    "\n",
    "In practice, the output weight matrix $W$ is often **tied** with the input token embedding matrix $\\mathbf{E}_{\\text{token}}$ as proposed by [<cite>Press & Wolf (2017)</cite>](https://arxiv.org/pdf/1608.05859):\n",
    "\n",
    "$$\n",
    "W = \\mathbf{E}_{\\text{token}}\n",
    "$$\n",
    "\n",
    "This means the same learned matrix is used:\n",
    "- At the input stage: as a **lookup table** to map token IDs to embeddings\n",
    "- At the output stage: as a **matrix** in a **real-valued projection** to compute vocabulary logits via matrix multiplication\n",
    "\n",
    "While the lookup table simply retrieves a row of $\\mathbf{E}_{\\text{token}}$ for each token ID, the output projection takes a full hidden vector and performs a dot product with **every row** of $\\mathbf{E}_{\\text{token}}$ .\n",
    "\n",
    "### Why tie weights?\n",
    "\n",
    "- **Parameter efficiency**: Tying reduces the number of parameters, especially with large vocabularies.\n",
    "- **Consistency**: The model learns a shared space for representing and predicting tokens.\n",
    "- **Regularization**: It encourages the model to reuse structure in a meaningful way.\n",
    "- **Empirical gains**: Tied embeddings have been shown to slightly improve performance in many NLP tasks.\n",
    "\n",
    "### One output per position\n",
    "\n",
    "During **training**, we compute logits for **every position** and apply a cross-entropy loss against the next token.  \n",
    "During **inference**, we typically only use the **last token’s output** to generate the next token step-by-step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b338df6",
   "metadata": {},
   "source": [
    "# For those of you that stuck till the end, here some questions to see if you got the gist of it!\n",
    "\n",
    "1. After mapping token ids to embeddings with positional information. Whats the shape of the result? \n",
    "2. How many trainable parameters do we have in total for embeddings and learned positional embeddings? \n",
    "3. What is the shape of the output in training/inference and why are/aren´t they different?\n",
    "4. Compute $\\mathbb{E}[QK^\\top]$, $\\mathrm{Var}[QK^\\top]$ and $\\mathrm{Var}\\left[\\frac{1}{\\sqrt{d}} QK^\\top\\right]$. Assume that all entries in $Q,K$ are i.i.d. with $N(0, 1)$.What can we see here and what does it justify when considering this as the input of the softmax function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc9319",
   "metadata": {},
   "source": [
    "# Answers\n",
    "\n",
    "1. Trivial right? But i wanted you to stick around! The size is $\\mathbb{R}^{c \\times d}$\n",
    "\n",
    "2. We have $d \\times v + d \\times c$ trainable parameters sice we only need positions for up to the maximum context lenght!\n",
    "\n",
    "3. This one is a bit more subtle.\n",
    "\n",
    "    During **training**, we don’t generate tokens one by one — that would be inefficient and wasteful. Instead, we input the **entire sequence** at once (often maximizing context length to fully utilize the GPU). \n",
    "    \n",
    "    **SIDENOTE** \n",
    "    \n",
    "    That means multiple documents may be present in a single input batch, sometimes separated by `<EOS>` tokens.\n",
    "\n",
    "    Now, this might raise a concern: _can the model attend across document boundaries?_ Technically yes — it **could** attend to unrelated tokens if they're still in the attention window. But in practice, models learn that `<EOS>` tokens are semantic boundaries, and they adapt by **suppressing attention** beyond them. It’s a learned behavior, not a hard constraint. \n",
    "    \n",
    "    **SIDENOTE END**\n",
    "\n",
    "    So what does training output look like?\n",
    "\n",
    "    - The model returns a tensor of shape:  \n",
    "    $$\n",
    "    \\mathbb{R}^{c \\times v}\n",
    "    $$  \n",
    "    where $c$ is the number of tokens (context length), and $v$ is the vocabulary size.\n",
    "    - Each row is the logits for predicting the **next token** at that position.\n",
    "    - We then apply cross-entropy loss against the next-token ground truth for **each position** in the input.\n",
    "\n",
    "    In contrast, **inference** is **autoregressive** — we start with a user input, compute all keys and values once for that prompt (per head and block!), and then generate one token at a time.\n",
    "\n",
    "    - For each new token, we compute just a **single** query vector.\n",
    "    - This query attends to the cached keys and values from both the prompt and any previously generated tokens.\n",
    "    - The model produces a **single output vector** of shape:  \n",
    "    $$\n",
    "    \\mathbb{R}^{v}\n",
    "    $$  \n",
    "    representing the logits for the next token.\n",
    "\n",
    "    And importantly, we update our **KV cache** with the new token’s key and value vectors — so we never recompute the entire sequence again, keeping inference efficient.\n",
    "\n",
    "4. Let's first tackle the expected value of $QK^\\top$. As we know by now this matrix captures how much each token \"attends to\" every other token, with shape $\\mathbb{R}^{c \\times c}$, where $c$ is the context length.\n",
    "\n",
    "    Each entry $(i, j)$ in $QK^\\top$ is the dot product between the $i$-th row of $Q$ and the $j$-th row of $K$, i.e., $\\mathbf{q}_i \\cdot \\mathbf{k}_j^\\top$. Since all entries in both $Q$ and $K$ are assumed to be i.i.d. samples from $\\mathcal{N}(0, 1)$, the expected value of each individual product $q_{i,l} \\cdot k_{j,l}$ is zero. \n",
    "    \n",
    "    **SIDENOTE** \n",
    "\n",
    "    Lets quickly talk about why this assumptions is actually quite reasonable. \n",
    "    - In **later layers**, the inputs to the attention mechanism come **directly from LayerNorm**, which normalizes each token vector to **zero mean and unit variance**. That makes the standard i.i.d. $\\mathcal{N}(0,1)$ approximation quite valid (especially early in training, before strong correlations emerge).\n",
    "    - Even in **initial layers**, if the model is initialized with common weight schemes  (Xavier or Kaiming), and inputs are roughly uncorrelated, the outputs of the linear projections used to compute $Q$ and $K$ will behave like i.i.d. zero-mean Gaussians due to the central limit effect from summing across input dimensions.\n",
    "    - Moreover, this is a **common theoretical simplification** used in analyzing attention mechanisms — not to model exact values, but to understand scaling behavior, such as why the $1/\\sqrt{d}$ factor is crucial.\n",
    "\n",
    "    **SIDENOTE END**\n",
    "\n",
    "    The dot product is a sum of these zero-mean terms, so:\n",
    "    $$\n",
    "    \\mathbb{E}[\\mathbf{q}_i \\cdot \\mathbf{k}_j^\\top] = 0\n",
    "    $$\n",
    "    and therefore:\n",
    "    $$\n",
    "    \\mathbb{E}[QK^\\top] = 0\n",
    "    $$\n",
    "\n",
    "    Now for the variance: Each entry in $QK^\\top$ is a sum of $d$ products of independent standard normals. Each term $q_{i,l} \\cdot k_{j,l}$ has:\n",
    "    - Mean 0\n",
    "    - Variance 1 (since $\\text{Var}(XY) = 1$ when $X, Y \\sim \\mathcal{N}(0, 1)$ independently)\n",
    "\n",
    "    So each entry in $QK^\\top$ has variance:\n",
    "    $$\n",
    "    \\mathrm{Var}[(QK^\\top)_{ij}] = \\sum_{l=1}^d \\mathrm{Var}(q_{i,l} \\cdot k_{j,l}) = d\n",
    "    $$\n",
    "\n",
    "    If we instead scale the dot product by $\\frac{1}{\\sqrt{d}}$, we get:\n",
    "    $$\n",
    "    \\mathrm{Var}\\left[\\left(\\frac{1}{\\sqrt{d}} QK^\\top\\right)_{ij}\\right] = \\frac{1}{d} \\cdot d = 1\n",
    "    $$\n",
    "\n",
    "    **Why does this matter for softmax?**\n",
    "    Well we all heard that before but here is a recap:\n",
    "\n",
    "    The softmax function becomes very sharp and unstable when input values are large, which often happens when the variance of the logits grows with dimensionality. Without scaling, the attention logits in $QK^\\top$ would have variance $d$, leading to:\n",
    "    - Large logits\n",
    "    - Exploding gradients\n",
    "    - Poor learning dynamics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
